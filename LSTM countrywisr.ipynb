{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv',index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location(arg):\n",
    "    state = arg[0]\n",
    "    country = arg[1]\n",
    "    loc = str(state)+str(country)\n",
    "    \n",
    "    return loc\n",
    "\n",
    "df['location'] = df[['Province_State','Country_Region']].apply(location,axis=1)\n",
    "df.drop(['Province_State','Country_Region'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df['location'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "36/36 [==============================] - ETA: 10s - loss: 4.4321e- - ETA: 1s - loss: 0.0408     - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.028 - 1s 17ms/step - loss: 0.0274\n",
      "Epoch 2/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 6ms/step - loss: 0.0059\n",
      "Epoch 3/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.007 - 0s 8ms/step - loss: 0.0069\n",
      "Epoch 4/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - 0s 7ms/step - loss: 0.0039\n",
      "Epoch 5/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 9.4079e-0 - ETA: 0s - loss: 0.0012    - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 6/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 8.4260e-0 - ETA: 0s - loss: 0.0021    - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - 0s 7ms/step - loss: 0.0024\n",
      "Epoch 7/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.7993e-0 - ETA: 0s - loss: 6.0640e-0 - ETA: 0s - loss: 0.0019    - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - 0s 7ms/step - loss: 0.0023\n",
      "Epoch 8/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 8.0444e-0 - ETA: 0s - loss: 7.7307e-0 - ETA: 0s - loss: 0.0016    - ETA: 0s - loss: 0.002 - 0s 7ms/step - loss: 0.0019\n",
      "Epoch 9/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 9.7610e-0 - ETA: 0s - loss: 0.0010    - ETA: 0s - loss: 0.001 - 0s 8ms/step - loss: 0.0013\n",
      "Epoch 10/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.3597e-0 - ETA: 0s - loss: 8.7149e-0 - ETA: 0s - loss: 9.4200e-0 - ETA: 0s - loss: 9.9494e-0 - ETA: 0s - loss: 0.0010    - 0s 7ms/step - loss: 9.6352e-04\n",
      "Epoch 11/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 9.5407e-0 - ETA: 0s - loss: 7.5769e-0 - 0s 8ms/step - loss: 8.5589e-04\n",
      "Epoch 12/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.7014e-0 - ETA: 0s - loss: 7.1361e-0 - ETA: 0s - loss: 8.0926e-0 - ETA: 0s - loss: 9.5403e-0 - ETA: 0s - loss: 0.0011    - 0s 7ms/step - loss: 0.0010\n",
      "Epoch 13/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.7219e-0 - ETA: 0s - loss: 8.1579e-0 - ETA: 0s - loss: 0.0010    - ETA: 0s - loss: 8.5853e-0 - ETA: 0s - loss: 8.8355e-0 - 0s 7ms/step - loss: 8.6982e-04\n",
      "Epoch 14/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 6.8408e-0 - ETA: 0s - loss: 8.2073e-0 - ETA: 0s - loss: 8.7975e-0 - ETA: 0s - loss: 7.6421e-0 - 0s 7ms/step - loss: 7.9871e-04\n",
      "Epoch 15/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.8338e-0 - ETA: 0s - loss: 4.4444e-0 - ETA: 0s - loss: 4.1576e-0 - ETA: 0s - loss: 5.0433e-0 - ETA: 0s - loss: 7.6958e-0 - 0s 8ms/step - loss: 7.5321e-04\n",
      "Epoch 16/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - 0s 7ms/step - loss: 0.0013\n",
      "Epoch 17/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 8.2819e-0 - ETA: 0s - loss: 6.5101e-0 - ETA: 0s - loss: 8.2981e-0 - ETA: 0s - loss: 7.2471e-0 - 0s 7ms/step - loss: 7.1400e-04\n",
      "Epoch 18/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 7.7972e-0 - ETA: 0s - loss: 8.4735e-0 - ETA: 0s - loss: 7.5888e-0 - 0s 8ms/step - loss: 7.8751e-04\n",
      "Epoch 19/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.8337e-0 - ETA: 0s - loss: 7.0684e-0 - ETA: 0s - loss: 7.2578e-0 - ETA: 0s - loss: 8.0399e-0 - ETA: 0s - loss: 7.9828e-0 - 0s 7ms/step - loss: 8.8239e-04\n",
      "Epoch 20/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 4.7222e-0 - ETA: 0s - loss: 0.0014    - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 9.9985e-0 - 0s 8ms/step - loss: 9.4095e-04\n",
      "Epoch 21/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 7.8840e-0 - ETA: 0s - loss: 4.6714e-0 - ETA: 0s - loss: 5.8835e-0 - ETA: 0s - loss: 7.2767e-0 - ETA: 0s - loss: 8.1996e-0 - 0s 7ms/step - loss: 7.7535e-04\n",
      "Epoch 22/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 5.1685e-0 - ETA: 0s - loss: 6.9512e-0 - ETA: 0s - loss: 7.2134e-0 - ETA: 0s - loss: 9.4631e-0 - ETA: 0s - loss: 8.8927e-0 - 0s 8ms/step - loss: 9.8764e-04\n",
      "Epoch 23/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.6989e-0 - ETA: 0s - loss: 0.0015    - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 9.8547e-0 - 0s 7ms/step - loss: 9.5569e-04\n",
      "Epoch 24/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 9.8863e-0 - ETA: 0s - loss: 0.0010    - ETA: 0s - loss: 9.7899e-0 - ETA: 0s - loss: 9.0058e-0 - 0s 7ms/step - loss: 8.1026e-04\n",
      "Epoch 25/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - 0s 8ms/step - loss: 0.0011\n",
      "Epoch 26/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.8769e-0 - ETA: 0s - loss: 0.0011    - ETA: 0s - loss: 6.8941e-0 - ETA: 0s - loss: 8.8657e-0 - ETA: 0s - loss: 7.4499e-0 - 0s 7ms/step - loss: 7.1198e-04\n",
      "Epoch 27/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 5.7312e-0 - ETA: 0s - loss: 5.8749e-0 - ETA: 0s - loss: 5.9393e-0 - ETA: 0s - loss: 7.8573e-0 - 0s 8ms/step - loss: 7.6449e-04\n",
      "Epoch 28/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 7.9917e-0 - ETA: 0s - loss: 8.1890e-0 - ETA: 0s - loss: 8.4648e-0 - 0s 7ms/step - loss: 8.2521e-04\n",
      "Epoch 29/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 9.3909e-0 - ETA: 0s - loss: 6.4090e-0 - ETA: 0s - loss: 7.4176e-0 - ETA: 0s - loss: 9.4802e-0 - 0s 8ms/step - loss: 9.4313e-04\n",
      "Epoch 30/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 9.9002e-0 - ETA: 0s - loss: 3.7409e-0 - ETA: 0s - loss: 5.3150e-0 - ETA: 0s - loss: 7.2782e-0 - ETA: 0s - loss: 7.5400e-0 - 0s 8ms/step - loss: 8.7508e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:10<00:20, 10.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "36/36 [==============================] - ETA: 10s - loss: 0.0000e+ - ETA: 1s - loss: 9.9148e-04 - ETA: 0s - loss: 0.0201    - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.031 - 1s 16ms/step - loss: 0.0304\n",
      "Epoch 2/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.008 - 0s 7ms/step - loss: 0.0085\n",
      "Epoch 3/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.004 - 0s 7ms/step - loss: 0.0040\n",
      "Epoch 4/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.015 - 0s 8ms/step - loss: 0.0138\n",
      "Epoch 5/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 7.2316e-0 - ETA: 0s - loss: 0.0014    - ETA: 0s - loss: 0.002 - 0s 7ms/step - loss: 0.0021\n",
      "Epoch 6/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 9.6760e-0 - ETA: 0s - loss: 0.0015    - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.003 - 0s 8ms/step - loss: 0.0043\n",
      "Epoch 7/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.031 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.003 - 0s 7ms/step - loss: 0.0031\n",
      "Epoch 8/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.8860e-0 - ETA: 0s - loss: 1.8140e-0 - ETA: 0s - loss: 0.0015    - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.002 - 0s 9ms/step - loss: 0.0023\n",
      "Epoch 9/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.5628e-0 - ETA: 0s - loss: 0.0016    - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - 0s 7ms/step - loss: 0.0011\n",
      "Epoch 10/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 9.2979e-0 - ETA: 0s - loss: 2.9836e-0 - ETA: 0s - loss: 6.8636e-0 - ETA: 0s - loss: 9.4867e-0 - ETA: 0s - loss: 7.9540e-0 - 0s 8ms/step - loss: 7.6076e-04\n",
      "Epoch 11/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 7.0092e-0 - ETA: 0s - loss: 2.7707e-0 - ETA: 0s - loss: 4.5208e-0 - ETA: 0s - loss: 5.9635e-0 - ETA: 0s - loss: 5.9212e-0 - 0s 8ms/step - loss: 7.0126e-04\n",
      "Epoch 12/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 6.1736e-0 - ETA: 0s - loss: 2.3984e-0 - ETA: 0s - loss: 1.9003e-0 - ETA: 0s - loss: 3.1652e-0 - ETA: 0s - loss: 4.7149e-0 - 0s 8ms/step - loss: 5.0714e-04\n",
      "Epoch 13/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.7561e-0 - ETA: 0s - loss: 5.5876e-0 - ETA: 0s - loss: 3.4216e-0 - ETA: 0s - loss: 4.2907e-0 - ETA: 0s - loss: 5.0255e-0 - 0s 7ms/step - loss: 4.4964e-04\n",
      "Epoch 14/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.0469e-0 - ETA: 0s - loss: 8.0880e-0 - ETA: 0s - loss: 5.7565e-0 - ETA: 0s - loss: 5.1135e-0 - ETA: 0s - loss: 5.1364e-0 - 0s 7ms/step - loss: 4.9682e-04\n",
      "Epoch 15/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.9294e-0 - ETA: 0s - loss: 4.6230e-0 - ETA: 0s - loss: 4.2453e-0 - ETA: 0s - loss: 4.4826e-0 - 0s 6ms/step - loss: 4.3450e-04\n",
      "Epoch 16/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 8.9223e-0 - ETA: 0s - loss: 4.3423e-0 - ETA: 0s - loss: 4.2801e-0 - ETA: 0s - loss: 4.4914e-0 - ETA: 0s - loss: 4.2631e-0 - 0s 7ms/step - loss: 3.8688e-04\n",
      "Epoch 17/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 4.0882e-0 - ETA: 0s - loss: 5.0248e-0 - ETA: 0s - loss: 4.0558e-0 - ETA: 0s - loss: 3.4018e-0 - ETA: 0s - loss: 3.3922e-0 - 0s 7ms/step - loss: 3.6442e-04\n",
      "Epoch 18/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 6.0158e-0 - ETA: 0s - loss: 3.6789e-0 - ETA: 0s - loss: 4.3191e-0 - ETA: 0s - loss: 3.0228e-0 - ETA: 0s - loss: 3.7381e-0 - 0s 8ms/step - loss: 3.6999e-04\n",
      "Epoch 19/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.3726e-0 - ETA: 0s - loss: 8.2402e-0 - ETA: 0s - loss: 5.0957e-0 - ETA: 0s - loss: 4.2980e-0 - ETA: 0s - loss: 3.5040e-0 - 0s 7ms/step - loss: 3.2169e-04\n",
      "Epoch 20/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 6.5759e-0 - ETA: 0s - loss: 2.7718e-0 - ETA: 0s - loss: 1.7012e-0 - ETA: 0s - loss: 1.7144e-0 - ETA: 0s - loss: 2.5199e-0 - 0s 7ms/step - loss: 2.5675e-04\n",
      "Epoch 21/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.4000e-0 - ETA: 0s - loss: 0.0011    - ETA: 0s - loss: 7.0859e-0 - ETA: 0s - loss: 8.0371e-0 - 0s 6ms/step - loss: 6.6390e-04\n",
      "Epoch 22/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 5.5090e-0 - ETA: 0s - loss: 1.8089e-0 - ETA: 0s - loss: 2.1148e-0 - ETA: 0s - loss: 8.9729e-0 - ETA: 0s - loss: 8.6261e-0 - 0s 7ms/step - loss: 9.2609e-04\n",
      "Epoch 23/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 8.5396e-0 - ETA: 0s - loss: 0.0010    - ETA: 0s - loss: 8.1246e-0 - ETA: 0s - loss: 6.8290e-0 - 0s 7ms/step - loss: 6.7363e-04\n",
      "Epoch 24/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 7.9892e-0 - ETA: 0s - loss: 2.9875e-0 - ETA: 0s - loss: 2.7374e-0 - ETA: 0s - loss: 3.2019e-0 - ETA: 0s - loss: 3.0195e-0 - 0s 8ms/step - loss: 3.3731e-04\n",
      "Epoch 25/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 7.9086e-0 - ETA: 0s - loss: 3.0714e-0 - ETA: 0s - loss: 3.8811e-0 - ETA: 0s - loss: 4.3639e-0 - 0s 6ms/step - loss: 4.1773e-04\n",
      "Epoch 26/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.0387e-0 - ETA: 0s - loss: 4.5869e-0 - ETA: 0s - loss: 2.7888e-0 - ETA: 0s - loss: 3.0401e-0 - ETA: 0s - loss: 4.3616e-0 - 0s 7ms/step - loss: 4.2399e-04\n",
      "Epoch 27/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.8331e-0 - ETA: 0s - loss: 1.1641e-0 - ETA: 0s - loss: 2.2904e-0 - ETA: 0s - loss: 5.9490e-0 - ETA: 0s - loss: 0.0021    - 0s 8ms/step - loss: 0.0022\n",
      "Epoch 28/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 7.4617e-0 - ETA: 0s - loss: 0.0014    - ETA: 0s - loss: 0.001 - 0s 7ms/step - loss: 0.0012\n",
      "Epoch 29/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.4193e-0 - ETA: 0s - loss: 3.1034e-0 - ETA: 0s - loss: 8.9527e-0 - ETA: 0s - loss: 0.0010    - ETA: 0s - loss: 0.001 - 0s 7ms/step - loss: 9.9379e-04\n",
      "Epoch 30/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 5.2754e-0 - ETA: 0s - loss: 2.4551e-0 - ETA: 0s - loss: 4.0088e-0 - ETA: 0s - loss: 3.3220e-0 - ETA: 0s - loss: 3.7083e-0 - 0s 7ms/step - loss: 5.2743e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:20<00:10, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "36/36 [==============================] - ETA: 14s - loss: 0.0000e+ - ETA: 1s - loss: 0.1225     - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.089 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.060 - 1s 19ms/step - loss: 0.0586\n",
      "Epoch 2/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.012 - 0s 7ms/step - loss: 0.0109\n",
      "Epoch 3/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 8ms/step - loss: 0.0052\n",
      "Epoch 4/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 0s 7ms/step - loss: 0.0075\n",
      "Epoch 5/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - 0s 7ms/step - loss: 0.0042\n",
      "Epoch 6/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.9153e-0 - ETA: 0s - loss: 0.0032    - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 7/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 6.7260e-0 - ETA: 0s - loss: 7.1278e-0 - ETA: 0s - loss: 0.0015    - ETA: 0s - loss: 0.001 - 0s 8ms/step - loss: 0.0018\n",
      "Epoch 8/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 7.9404e-0 - ETA: 0s - loss: 0.0018    - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.001 - 0s 8ms/step - loss: 0.0020\n",
      "Epoch 9/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.1809e-0 - ETA: 0s - loss: 0.0021    - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - 0s 8ms/step - loss: 0.0014\n",
      "Epoch 10/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.0556e-0 - ETA: 0s - loss: 7.5745e-0 - ETA: 0s - loss: 0.0015    - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - 0s 7ms/step - loss: 0.0014\n",
      "Epoch 11/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.0391e-0 - ETA: 0s - loss: 0.0010    - ETA: 0s - loss: 9.5613e-0 - ETA: 0s - loss: 0.0012    - ETA: 0s - loss: 0.001 - 0s 8ms/step - loss: 0.0017\n",
      "Epoch 12/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.4922e-0 - ETA: 0s - loss: 0.0019    - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - 0s 7ms/step - loss: 0.0014\n",
      "Epoch 13/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.0070e-0 - ETA: 0s - loss: 7.6852e-0 - ETA: 0s - loss: 0.0016    - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - 0s 7ms/step - loss: 0.0012\n",
      "Epoch 14/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.4266e-0 - ETA: 0s - loss: 7.7630e-0 - ETA: 0s - loss: 9.2100e-0 - ETA: 0s - loss: 6.7596e-0 - ETA: 0s - loss: 0.0012    - 0s 8ms/step - loss: 0.0012\n",
      "Epoch 15/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 4.5234e-0 - ETA: 0s - loss: 0.0033    - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.001 - 0s 8ms/step - loss: 0.0018\n",
      "Epoch 16/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.9296e-0 - ETA: 0s - loss: 9.0370e-0 - ETA: 0s - loss: 5.4348e-0 - ETA: 0s - loss: 9.8566e-0 - ETA: 0s - loss: 0.0013    - 0s 7ms/step - loss: 0.0013\n",
      "Epoch 17/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - 0s 8ms/step - loss: 0.0014\n",
      "Epoch 18/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 7.1740e-0 - ETA: 0s - loss: 5.7471e-0 - ETA: 0s - loss: 6.3223e-0 - ETA: 0s - loss: 8.0107e-0 - ETA: 0s - loss: 9.7778e-0 - 0s 8ms/step - loss: 9.1330e-04\n",
      "Epoch 19/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.7813e-0 - ETA: 0s - loss: 7.7238e-0 - ETA: 0s - loss: 4.4929e-0 - ETA: 0s - loss: 5.2035e-0 - ETA: 0s - loss: 8.1513e-0 - 0s 7ms/step - loss: 8.5088e-04\n",
      "Epoch 20/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 6.4486e-0 - ETA: 0s - loss: 3.5165e-0 - ETA: 0s - loss: 6.5428e-0 - ETA: 0s - loss: 6.6252e-0 - 0s 8ms/step - loss: 8.7601e-04\n",
      "Epoch 21/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 9.5579e-0 - ETA: 0s - loss: 0.0010    - 0s 8ms/step - loss: 0.0010\n",
      "Epoch 22/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 8.7463e-0 - ETA: 0s - loss: 9.3674e-0 - ETA: 0s - loss: 0.0011    - ETA: 0s - loss: 0.001 - 0s 7ms/step - loss: 9.5707e-04\n",
      "Epoch 23/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.6956e-0 - ETA: 0s - loss: 3.9645e-0 - ETA: 0s - loss: 8.3016e-0 - ETA: 0s - loss: 7.6411e-0 - ETA: 0s - loss: 7.9280e-0 - 0s 8ms/step - loss: 8.4024e-04\n",
      "Epoch 24/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 9.3050e-0 - ETA: 0s - loss: 7.1071e-0 - ETA: 0s - loss: 7.6485e-0 - 0s 8ms/step - loss: 6.9636e-04\n",
      "Epoch 25/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.7071e-0 - ETA: 0s - loss: 8.6851e-0 - ETA: 0s - loss: 8.1262e-0 - ETA: 0s - loss: 8.5521e-0 - ETA: 0s - loss: 7.9189e-0 - 0s 7ms/step - loss: 7.7342e-04\n",
      "Epoch 26/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.4359e-0 - ETA: 0s - loss: 0.0012    - ETA: 0s - loss: 8.7262e-0 - ETA: 0s - loss: 7.5797e-0 - ETA: 0s - loss: 8.0666e-0 - 0s 8ms/step - loss: 7.2435e-04\n",
      "Epoch 27/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 8.4012e-0 - ETA: 0s - loss: 9.4873e-0 - ETA: 0s - loss: 7.2778e-0 - ETA: 0s - loss: 7.8622e-0 - ETA: 0s - loss: 7.4337e-0 - 0s 8ms/step - loss: 7.0186e-04\n",
      "Epoch 28/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.2499e-0 - ETA: 0s - loss: 7.6352e-0 - ETA: 0s - loss: 5.1408e-0 - ETA: 0s - loss: 6.3517e-0 - ETA: 0s - loss: 6.6669e-0 - 0s 7ms/step - loss: 6.1695e-04\n",
      "Epoch 29/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.6264e-0 - ETA: 0s - loss: 7.4918e-0 - ETA: 0s - loss: 7.5849e-0 - ETA: 0s - loss: 6.6977e-0 - ETA: 0s - loss: 7.4488e-0 - 0s 7ms/step - loss: 7.1118e-04\n",
      "Epoch 30/30\n",
      "36/36 [==============================] - ETA: 0s - loss: 5.7660e-0 - ETA: 0s - loss: 8.5507e-0 - ETA: 0s - loss: 8.2027e-0 - ETA: 0s - loss: 7.8776e-0 - ETA: 0s - loss: 8.3524e-0 - 0s 7ms/step - loss: 7.5402e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:30<00:00, 10.29s/it]"
     ]
    }
   ],
   "source": [
    "Res = None\n",
    "ctr = 0\n",
    "for i in tqdm(locations):\n",
    "    df_train = df[df['location']==i].drop('location',axis=1)\n",
    "    df_train['Date'] = pd.to_datetime(df_train['Date'],format='%d-%m-%Y')\n",
    "    df_train.index = df_train['Date']\n",
    "    df_train.drop('Date',inplace=True,axis=1)\n",
    "    df_train.index.freq = 'D'\n",
    "    \n",
    "    Res = pd.concat([Res,df_train.iloc[-13:]],axis=0)\n",
    "    train = df_train.copy()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train)\n",
    "    new_train = scaler.transform(train)\n",
    "    length = 12\n",
    "    batch = 2\n",
    "    generator = TimeseriesGenerator(new_train,new_train,length=length,batch_size=batch)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100,input_shape=(length,new_train.shape[1])))\n",
    "    model.add(Dense(new_train.shape[1]))\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    model.fit_generator(generator,epochs=30)\n",
    "    \n",
    "    first_eval_batch = new_train[-length:]\n",
    "    first_eval_batch = first_eval_batch.reshape((1, length, new_train.shape[1]))\n",
    "    n_features = new_train.shape[1]\n",
    "    test_predictions = []\n",
    "\n",
    "    first_eval_batch = new_train[-length:]\n",
    "    current_batch = first_eval_batch.reshape((1, length, n_features)) \n",
    "\n",
    "    for i in range(30):\n",
    "        current_pred = model.predict(current_batch)[0]\n",
    "        test_predictions.append(current_pred) \n",
    "        current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n",
    "        \n",
    "    pred = scaler.inverse_transform(test_predictions)\n",
    "    cases = [i[0] for i in pred]\n",
    "    death = [i[1] for i in pred]\n",
    "    \n",
    "    new = pd.DataFrame(cases,death,columns=['Fatalities']).reset_index()\n",
    "    new.columns = ['Fatalities','ConfirmedCases']\n",
    "    \n",
    "    Res = pd.concat([Res,new],axis=0)\n",
    "    Res = Res.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Res[['ConfirmedCases','Fatalities']].to_csv(\"submit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
